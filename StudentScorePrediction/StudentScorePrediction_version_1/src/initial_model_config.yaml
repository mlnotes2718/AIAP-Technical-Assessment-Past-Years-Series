# initial_model_config.yaml


#######################################################################################
### User Guide:                                                                  ######
#######################################################################################
### There are 5 main configurations
### They are CV_SIZE, SAMPLE_SIZE, MODEL_LIST, MODELS and PARAMS
### CV_SIZE : This allow us to set the size from cross validation dataset
### SAMPLE_SIZE : If you dataset is very large, you can reduce the taining dataset.
### MODEL_LIST : Use this setting to select which model to run, this way we can keep 
### the settings of all other models.
### MODELS : Use this setting to construct the pipelien for each model.
### PARAMS : Use this setting to define which parameters to run for each model
###
### Please not that this is no grid search, you can set the parameters with a wide 
### range. You can see the training score and test score of each combination.
### Use this tool to set a large range, look at the score and narrow down the search 
### parameters in preparation for grid search.
#######################################################################################

#############################################################################
### Settings Parameters
#############################################################################

# Define the size of cross validation dataset, range from 0.1 to 0.9
CV_SIZE : 0.2

# Define Sample size for large dataset
# If your dataset is very large, you can set the sample size
# Please note that the system will extract the CV dataset for testing first
# SAMPLE SIZE will only applied to the remaining training data after the CV Split
# Set from 0.1 to 1.0. 1.0 means not reducing teh dataset size.
SAMPLE_SIZE : 0.6

# Define which model to run
# The MODELS_LIST will define which model to run so there is no need to delete any model configuration

#MODELS_LIST : ['polynomial', 'ridge', 'decision_tree', 'knn_regressor', 'svr', 'rf_regressor', 'bg_regressor', 'xgboost']
#MODELS_LIST : ['decision_tree','rf_regressor', 'bg_regressor', 'xgboost']
MODELS_LIST : ['ridge', 'decision_tree']

# Define models pipelines that include polynomial features and scaling.
# For feature scaling we have StandardScaler and MinMaxScaler
MODELS : 
  'polynomial':
    - 'poly': PolynomialFeatures
    - 'scaler': StandardScaler
    - 'polynomial': LinearRegression
  'ridge':
    - 'poly': PolynomialFeatures
    - 'scaler': StandardScaler
    - 'ridge': Ridge
  'decision_tree':
    - 'decision_tree': DecisionTreeRegressor
  'knn_regressor':
    - 'poly': PolynomialFeatures
    - 'scaler': StandardScaler
    - 'knn_regressor': KNeighborsRegressor        
  'svr':
    - 'poly': PolynomialFeatures
    - 'scaler': StandardScaler
    - 'svr': SVR       
  'rf_regressor': 
    - 'rf_regressor': RandomForestRegressor
  'gb_regressor': 
    - 'gb_regressor': GradientBoostingRegressor
  'xgboost': 
    - 'scaler': StandardScaler
    - 'xgboost': XGBRegressor



# Define hyperparameters for each model, including polynomial degrees
PARAMS : {
    'polynomial': {
        'poly__degree': [2],              
    },
    'ridge': {
        'poly__degree': [2],             
        'ridge__alpha': [0.05, 0.1, 0.5],
        'ridge__solver': ['auto', 'svd']
    },
    'decision_tree': {
        'decision_tree__max_depth': [10, 20, 30, 40, 50],
        'decision_tree__min_samples_split': [70, 80, 90, 100],
        'decision_tree__min_samples_leaf': [1, 3, 5, 7]
    },
    'knn_regressor': {
        'poly__degree': [2],
        'knn_regressor__n_neighbors': [18, 20, 22],
        'knn_regressor__weights': ['distance'],
        'knn_regressor__p': [1]
    },
    'svr': {
        'poly__degree': [2],
        'svr__kernel': ['linear'],
        'svr__C': [0.1, 1, 3, 5],
        'svr__epsilon': [0.1, 1, 3, 5],
        'svr__gamma': [0.000001, 0.00001, 0.0001]
    },
    'rf_regressor':{
        'rf_regressor__n_estimators': [500, 1000],
        'rf_regressor__max_depth': [50, 100, 500],
        'rf_regressor__min_samples_split': [3, 5, 10, 15],
        'rf_regressor__min_samples_leaf': [1, 3],
        'rf_regressor__bootstrap': [True]        
    },
    'gb_regressor':{
        'gb_regressor__n_estimators': [400, 500],
        'gb_regressor__learning_rate': [0.05, 0.08, 0.1],
        'gb_regressor__max_depth': [4, 5, 8],
        #'gb_regressor__min_samples_split': [10, 15, 20],
        #'gb_regressor__min_samples_leaf': [1, 2, 4, 8],
        #'gb_regressor__subsample': [0.7, 0.85, 1.0],
        #'gb_regressor__loss': ['squared_error', 'huber']        
    },
    'xgboost': {
        "xgboost__n_estimators": [100, 200, 500],
        #"xgboost__learning_rate": [0.01, 0.05, 0.1, 0.2, 0.5],
        #"xgboost__max_depth": [3, 5, 10, 15],
        #"xgboost__subsample": [0.2, 0.5, 1.0],
        #"xgboost__colsample_bytree": [0.1, 0.5, 1.0],
        #"xgboost__min_child_weight": [1, 50, 100]
    }
}
